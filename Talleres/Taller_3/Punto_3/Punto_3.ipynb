{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de documentos: 491\n"
     ]
    }
   ],
   "source": [
    "# Abre el documento de Word\n",
    "doc_ciencia = docx.Document(\"ExportacionWordCREA_Ciencia.doc\")\n",
    "doc_deporte = docx.Document(\"ExportacionWordCREA_Deporte.doc\")\n",
    "doc_viaje = docx.Document(\"ExportacionWordCREA_Viaje.doc\")\n",
    "\n",
    "archivos = [doc_ciencia, doc_deporte, doc_viaje]\n",
    "\n",
    "# Inicializa listas para los títulos y textos\n",
    "titulos = []\n",
    "documentos = []\n",
    "\n",
    "# Inicializa una bandera para determinar si estamos en una sección de título o texto\n",
    "en_titulo = True\n",
    "\n",
    "# Recorre el contenido del documento\n",
    "for doc in archivos:\n",
    "    for paragraph in doc.paragraphs[2:]:\n",
    "        if paragraph.text:  # Verifica si el párrafo no está vacío\n",
    "            if en_titulo:\n",
    "                titulos.append(paragraph.text)\n",
    "            else:\n",
    "                documentos.append(paragraph.text)\n",
    "            # Cambia la bandera en_titulo en cada iteración\n",
    "            en_titulo = not en_titulo\n",
    "\n",
    "print(f\"Numero de documentos: {len(documentos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     000  10  100  11  112  12  120  124  13  1300  ...  óseo  última   \n",
      "0      0   0    0   0    0   0    0    0   0     0  ...     0       0  \\\n",
      "1      0   0    0   0    0   0    0    0   0     0  ...     0       0   \n",
      "2      0   0    0   0    0   0    0    0   0     0  ...     0       0   \n",
      "3      0   0    0   0    0   0    0    0   0     0  ...     0       0   \n",
      "4      0   0    0   0    0   0    0    0   0     0  ...     0       0   \n",
      "..   ...  ..  ...  ..  ...  ..  ...  ...  ..   ...  ...   ...     ...   \n",
      "486    0   0    0   0    0   0    0    0   0     0  ...     0       1   \n",
      "487    0   0    0   0    0   0    0    0   0     0  ...     0       0   \n",
      "488    0   0    0   0    0   0    0    0   0     0  ...     0       0   \n",
      "489    0   0    0   0    0   0    0    0   0     0  ...     0       0   \n",
      "490    0   0    0   0    0   0    0    0   0     0  ...     0       0   \n",
      "\n",
      "     últimament  últimamente  último  últimos  única  únicament  único  útil  \n",
      "0             0            0       0        0      0          0      0     0  \n",
      "1             0            0       0        0      0          0      0     0  \n",
      "2             0            0       0        0      0          0      0     0  \n",
      "3             0            0       0        0      0          0      0     0  \n",
      "4             0            0       0        0      0          0      0     0  \n",
      "..          ...          ...     ...      ...    ...        ...    ...   ...  \n",
      "486           0            0       0        0      0          0      0     0  \n",
      "487           0            0       0        0      0          0      0     0  \n",
      "488           0            0       0        0      0          0      0     0  \n",
      "489           0            0       0        0      1          0      0     0  \n",
      "490           0            0       0        0      0          0      0     0  \n",
      "\n",
      "[491 rows x 8176 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define custom preprocessing function\n",
    "def custom_preprocess(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    \n",
    "    # Tokenization, lowercase, stopword removal, and stemming\n",
    "    words = text.lower().split()\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Create a CountVectorizer with custom preprocessing\n",
    "count_vectorizer = CountVectorizer(preprocessor=custom_preprocess)\n",
    "\n",
    "# Fit and transform the documents using the vectorizer\n",
    "count_matrix = count_vectorizer.fit_transform(documentos)\n",
    "\n",
    "# Convert the term frequency matrix to a Pandas DataFrame\n",
    "term_frequency_df = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the term frequency matrix\n",
    "print(term_frequency_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     000   10  100   11  112   12  120  124   13  1300  ...  óseo    última   \n",
      "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0  0.000000  \\\n",
      "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0  0.000000   \n",
      "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0  0.000000   \n",
      "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0  0.000000   \n",
      "4    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0  0.000000   \n",
      "..   ...  ...  ...  ...  ...  ...  ...  ...  ...   ...  ...   ...       ...   \n",
      "486  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0  0.132116   \n",
      "487  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0  0.000000   \n",
      "488  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0  0.000000   \n",
      "489  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0  0.000000   \n",
      "490  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0  0.000000   \n",
      "\n",
      "     últimament  últimamente  último  últimos     única  únicament  único   \n",
      "0           0.0          0.0     0.0      0.0  0.000000        0.0    0.0  \\\n",
      "1           0.0          0.0     0.0      0.0  0.000000        0.0    0.0   \n",
      "2           0.0          0.0     0.0      0.0  0.000000        0.0    0.0   \n",
      "3           0.0          0.0     0.0      0.0  0.000000        0.0    0.0   \n",
      "4           0.0          0.0     0.0      0.0  0.000000        0.0    0.0   \n",
      "..          ...          ...     ...      ...       ...        ...    ...   \n",
      "486         0.0          0.0     0.0      0.0  0.000000        0.0    0.0   \n",
      "487         0.0          0.0     0.0      0.0  0.000000        0.0    0.0   \n",
      "488         0.0          0.0     0.0      0.0  0.000000        0.0    0.0   \n",
      "489         0.0          0.0     0.0      0.0  0.121226        0.0    0.0   \n",
      "490         0.0          0.0     0.0      0.0  0.000000        0.0    0.0   \n",
      "\n",
      "     útil  \n",
      "0     0.0  \n",
      "1     0.0  \n",
      "2     0.0  \n",
      "3     0.0  \n",
      "4     0.0  \n",
      "..    ...  \n",
      "486   0.0  \n",
      "487   0.0  \n",
      "488   0.0  \n",
      "489   0.0  \n",
      "490   0.0  \n",
      "\n",
      "[491 rows x 8255 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define custom preprocessing function\n",
    "def custom_preprocess(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Tokenization, lowercase, stopword removal, and stemming\n",
    "    words = text.lower().split()\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Create a TfidfVectorizer with custom preprocessing\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=custom_preprocess)\n",
    "\n",
    "# Fit and transform the documents using the vectorizer\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documentos)\n",
    "\n",
    "# Convert the TF-IDF matrix to a Pandas DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "print(tfidf_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
