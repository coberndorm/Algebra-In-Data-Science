{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "Sea $\\Omega$ un conjunto finito. Demuestre que la distancia Jaccard en $\\wp(\\Omega)$ definida como\n",
    "$$\n",
    "d(A, B):=\\frac{|A \\triangle B|}{|A \\cup B|},\n",
    "$$\n",
    "define una métrica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demostracion\n",
    "\n",
    "Para demostrar que la distancia de Jaccard es una metrica se deben demostrar que cumple las siguentes propiedades:\n",
    "\n",
    "1. No-negatividad: $d(A,B) \\geq 0$ para todo $A,B \\in \\wp(\\Omega)$\n",
    "2. Coincidencia: $d(A,B) = 0$ si y solo si $A = B$\n",
    "3. Simetria: $d(A,B) = d(B,A)$ para todo $A,B \\in \\wp(\\Omega)$\n",
    "4. Desigualdad Triangular: $d(A,B) \\leq d(A,C) + d(C,B)$ para todo $A,B,C \\in \\wp(\\Omega)$\n",
    "\n",
    "Demostacion:\n",
    "\n",
    "1. **No-negatividad**:\n",
    "    $$\n",
    "    d(A,B) = \\frac{|A \\triangle B|}{|A \\cup B|} \\geq 0\n",
    "    $$\n",
    "    Dado a que tanto el numerador como el denominador son positivos\n",
    "\n",
    "2. **Coincidencia**:\n",
    "\n",
    "$$\n",
    "d(A,B) = 0 \\iff |A \\triangle B| = 0 \\iff A = B\n",
    "$$\n",
    "\n",
    "3. **Simetria**:\n",
    "\n",
    "$$\n",
    "d(A,B) = \\frac{|A \\triangle B|}{|A \\cup B|} = \\frac{|B \\triangle A|}{|B \\cup A|} = d(B,A)\n",
    "$$\n",
    "\n",
    "4. **Desigualdad Triangular**:\n",
    "\n",
    "    $$\n",
    "    d(A,B) = \\frac{|A \\triangle B|}{|A \\cup B|} \\leq \\frac{|A \\triangle C| + |C \\triangle B|}{|A \\cup C| + |C \\cup B|} \\leq \\frac{|A \\triangle C|}{|A \\cup C|} + \\frac{|C \\triangle B|}{|C \\cup B|} = d(A,C) + d(C,B)\n",
    "    $$\n",
    "\n",
    "Por tanto la distancia de Jaccard si es una metrica\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.\n",
    " Sea $Q$ una matriz tal que $Q^t Q=I$, demuestre que $Q$ preserva magnitudes y ángulos respecto al producto punto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demostracion\n",
    "\n",
    "Dado que $Q^t Q = I$, la matriz $Q$ es una matriz ortogonal.\n",
    "\n",
    "**Preservación de magnitudes:**\n",
    "\n",
    "Para demostrar que $Q$ preserva magnitudes, consideremos un vector $v$, y su imágenes bajo la aplicación de $Q$, denotada como $Qv$. Queremos demostrar que $\\|Qv\\| = \\|v\\|.\n",
    "\n",
    "$$||Qv||^2 = (Qv)^t(Qv) = v^t Q^t Qv = v^t Iv = v^t v = ||v||^2$$\n",
    "\n",
    "Por lo tanto, $Q$ preserva magnitudes.\n",
    "\n",
    "**Preservación de ángulos:**\n",
    "\n",
    "Para demostrar que $Q$ preserva ángulos, consideremos dos vectores $v$ y $w$ con ángulo $\\theta$ entre ellos. Queremos demostrar que el ángulo entre $Qv$ y $Qw$ también es $\\theta$.\n",
    "\n",
    "$$ \\cos(\\theta) = \\frac{v \\cdot w}{||v|| \\cdot ||w||} $$\n",
    "\n",
    "Ahora, consideremos $Qv \\cdot Qw$:\n",
    "\n",
    "$$ Qv \\cdot Qw = (Qv)^t(Qw) = v^t Q^t Qw = v^t I w = v \\cdot w $$\n",
    "\n",
    "Dividimos ambos lados por \\(||Qv|| \\cdot ||Qw||\\), por la preservacion de magnitudes:\n",
    "\n",
    "$$ \\cos(\\theta) = \\frac{Qv \\cdot Qw}{||Qv|| \\cdot ||Qw||} $$\n",
    "\n",
    "Por lo tanto $Q$ preserva angulos.\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. \n",
    "Demuestre que la norma matricial inducida por la norma euclidiana es igual a el mayor valor singular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demostracion\n",
    "\n",
    "Sea A una matriz de tamaño $m \\times n$. La norma inducida se define como:\n",
    "\n",
    "$$||A|| = \\sup \\{ ||Ax|| : ||x|| = 1 \\}$$\n",
    "\n",
    "Ahora, consideremos lo siguiente:\n",
    "\n",
    "$$||Ax||^2 = (Ax)^T(Ax) = x^T(A^TA)x$$\n",
    "\n",
    "Dado que $A^TA$ es una matriz simétrica definida positiva, su descomposicion en valores singulares tiene la siguiente forma $ A = V \\Sigma V^T$, donde:\n",
    "- $\\Sigma$ es una matriz diagonal de tamaño $m \\times n$ con los valores singulares en la diagonal, y\n",
    "- $V$ es una matriz ortogonal de tamaño $n \\times n$.\n",
    "\n",
    "Por tanto se tiene que:\n",
    "$$x^T(A^TA)x = x^T (V \\Sigma V^T)x$$\n",
    "Ahora definimos $V^T x = y$ obtenemos la expresion:\n",
    "$$x^T V \\Sigma V^T x = y^T \\Sigma y =  \\sum_i \\lambda_i y_i^2$$\n",
    "Esto implica que:\n",
    "$$\\|Ax\\|^2 \\leq \\lambda_{max} y^Ty = \\lambda_{max} x^TV^TVx = \\lambda_{max} $$\n",
    "Esto anterior se debe a que $V^TV = I$ al $V$ ser ortogonal y por la definición de $x$ tenemos que $\\|x\\| = x^Tx = 1$\n",
    "\n",
    "Para finalizar la demostracion tomamos a $x = v_{max}$ donde $v_{max}$ es el autovector correspondiente al mayor autovalor. De esta manera se obtiene que $\\|Ax\\|^2 = \\lambda_{max}$. \n",
    "\n",
    "Finalmente tenemos entonces que $\\|A\\|_2 = \\sqrt{\\lambda_{max} (A^TA)}$. O que la norma eculideana inducida es igual al mayor valor singular de una matriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implemente un algoritmo para calcular los valores y vectores propios de una matriz simétrica $A$ a partir de la siguiente idea:\n",
    "Inicie con $A_0=A$, en el paso $k$ calcule la descomposición QR de $A_k=Q_k R_k$ y defina $A_{k+1}=R_k Q_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def symmetrical_matrix_eigenvectors(matrix, tolerance):\n",
    "    A_k = matrix\n",
    "    Q_total = np.eye(A_k.shape[0])\n",
    "\n",
    "    while True:     \n",
    "        Q,R = np.linalg.qr(A_k)\n",
    "\n",
    "        if np.linalg.norm(A_k - np.dot(R,Q)) < tolerance:\n",
    "            return np.diag(A_k), - Q_total\n",
    "        \n",
    "        A_k = np.dot(R,Q)\n",
    "        Q_total = np.dot(Q_total, Q)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2,3,4],\n",
    "              [4,5,6,7],\n",
    "              [7,8,7,6],\n",
    "              [4,2,3,4]]) * 0.1\n",
    "\n",
    "A = A @ A.T\n",
    "\n",
    "eig_val, eig_vec = symmetrical_matrix_eigenvectors(A, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.86, 0.1 , 0.  , 0.03]),\n",
       " array([[-0.26, -0.59,  0.72, -0.25],\n",
       "        [-0.57, -0.48, -0.65, -0.15],\n",
       "        [-0.71,  0.64,  0.2 , -0.22],\n",
       "        [-0.33, -0.09,  0.13,  0.93]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.linalg.eig(A)[0],2), np.round(np.linalg.eig(A)[1],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.86, 0.1 , 0.03, 0.  ]),\n",
       " array([[-0.26, -0.59, -0.25,  0.72],\n",
       "        [-0.57, -0.48, -0.15, -0.65],\n",
       "        [-0.71,  0.64, -0.22,  0.2 ],\n",
       "        [-0.33, -0.09,  0.93,  0.13]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(eig_val,2), np.round(eig_vec,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este algoritmo es como una version mas sofisticada del algoritmo de potencias hecho en talleres anteriories. Recordemos la definicion de los autovalores y autovectores:\n",
    "$$ A v = A \\lambda$$ \n",
    "donde $v$ es un autovector y $\\lambda$ un autovalor. Como se utiliza descomposicion QR para renormalizar Y ortogonalizar la matriz cada vez se esta funcionando con una base de vectores completa. Tambien hay que considerar que estamos trabajando con una matriz simetrica $A$, por lo que al converger se debe obtener la siguiente igualdad $A Q = Q \\Lambda$ donde $\\Lambda$ es una matriz diagonal con los autovalores en la diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A*Q: \n",
      " [[-1.01 -0.06 -0.01  0.  ]\n",
      " [-2.19 -0.05 -0.01 -0.  ]\n",
      " [-2.74  0.06 -0.01 -0.  ]\n",
      " [-1.27 -0.01  0.03 -0.  ]]\n",
      "Q*lambda: \n",
      " [[-1.01 -0.06 -0.01  0.  ]\n",
      " [-2.19 -0.05 -0.01 -0.  ]\n",
      " [-2.74  0.06 -0.01  0.  ]\n",
      " [-1.27 -0.01  0.03  0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"A*Q: \\n\",  np.round(A@eig_vec,2))\n",
    "print(\"Q*lambda: \\n\", np.round(eig_vec@np.diag(eig_val),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El metodo funciona debido a que:\n",
    "1. **Descomposición QR**: En cada iteración del método, se realiza una descomposición QR de la matriz $A_k$​. La descomposición QR descompone la matriz original en una matriz ortogonal Q y una matriz triangular superior R.\n",
    "\n",
    "2. **Producto R_kQ_k**: Debido a la naturaleza de la descomposición QR, el producto $R_kQ_k$​ es equivalente a una transformación de similaridad que conserva los autovalores. Por tanto, los autovalores de $A_k$​ son los mismos que los de $R_kQ_k$​.\n",
    "\n",
    "3. **Convergencia a una matriz triangular**: Después de varias iteraciones, Ak tiende a converger hacia una matriz triangular superior. En una matriz triangular superior, los autovalores están en la diagonal principal, lo que facilita su identificación. Esto se debe a que se tienden a minimizar los valores no diagonales en la matriz $R$ de tal manera que esta converge a los autovalores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Cálcule los valores propios y los valores singulares para las siguientes matrices:\n",
    "$$\n",
    "A=\\left(\\begin{array}{llll}\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 2 & 0 \\\\\n",
    "0 & 0 & 0 & 3 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{array}\\right) \\quad B=\\left(\\begin{array}{cccc}\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 2 & 0 \\\\\n",
    "0 & 0 & 0 & 3 \\\\\n",
    "0.001 & 0 & 0 & 0\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0,1,0,0],\n",
    "             [0,0,2,0],\n",
    "             [0,0,0,3],\n",
    "             [0,0,0,0]])\n",
    "\n",
    "B = np.array([[0,1,0,0],\n",
    "            [0,0,2,0],\n",
    "            [0,0,0,3],\n",
    "            [0.001,0,0,0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores Singulares de A: [3. 2. 1. 0.]\n",
      "Valores Singulares de B: [3.e+00 2.e+00 1.e+00 1.e-03]\n"
     ]
    }
   ],
   "source": [
    "singular_values_A = np.linalg.svd(A, compute_uv=False)\n",
    "singular_values_B = np.linalg.svd(B, compute_uv=False)\n",
    "\n",
    "print(\"Valores Singulares de A:\", singular_values_A)\n",
    "print(\"Valores Singulares de B:\", singular_values_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ambas matrices tienen 0 en todos los valores de cada fila menos uno, los valores singulares son los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores Propios de A: [0. 0. 0. 0.]\n",
      "Valores Propios de B: [-0.28+0.j   -0.  +0.28j -0.  -0.28j  0.28+0.j  ]\n"
     ]
    }
   ],
   "source": [
    "eigenvalues_A = np.round(np.linalg.eigvals(A), 2)\n",
    "eigenvalues_B = np.round(np.linalg.eigvals(B), 2)\n",
    "\n",
    "print(\"Valores Propios de A:\", eigenvalues_A)\n",
    "print(\"Valores Propios de B:\", eigenvalues_B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como no es una matriz diagonal y tiene tanto 0 los valores obtenidos son imaginarios para B y para A por fuera de un rango menor que el tamaño y la primera fila es 0 en su totalidad, los autovalores son 0, sin embargo si se cambian las matrices a que sean diagonales mediante un cambio de la ultima fila a la primera, de la siguiente forma, se pueden calcular los autovalores y estos seran iguales a los valores singulares calculados anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores Propios de A: [0. 1. 2. 3.]\n",
      "Valores Propios de B: [1.e-03 1.e+00 2.e+00 3.e+00]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[0,0,0,0],\n",
    "              [0,1,0,0],\n",
    "             [0,0,2,0],\n",
    "             [0,0,0,3]])\n",
    "\n",
    "B = np.array([[0.001,0,0,0],\n",
    "              [0,1,0,0],\n",
    "              [0,0,2,0],\n",
    "              [0,0,0,3]])\n",
    "\n",
    "eigenvalues_A = np.linalg.eigvals(A)\n",
    "eigenvalues_B = np.linalg.eigvals(B)\n",
    "\n",
    "print(\"Valores Propios de A:\", eigenvalues_A)\n",
    "print(\"Valores Propios de B:\", eigenvalues_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taller 1, Problema 2\n",
    "\n",
    "A partir del algoritmo de k-vecinos más cercanos cree un mo delo que clasifique fotografías según el género de la p ersona retratada. Programar otro metodo para clasificar fotografias segun su genero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esto se deberia correr en colab\n",
    "\n",
    "!mkdir ~/.kaggle\n",
    "!touch ~/.kaggle/kaggle.json\n",
    "\n",
    "api_token = {'username':'dxninob','key':'f4d11e7051a81b59f2e5b57809e62332'}\n",
    "\n",
    "import json\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
    "    json.dump(api_token, file)\n",
    "\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d cashutosh/gender-classification-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q /content/gender-classification-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Función para leer dataset de imagenes\n",
    "def get_images(path):\n",
    "    X = []\n",
    "    y = []\n",
    "    # Iterar por directorios\n",
    "    for dir in os.listdir(path):\n",
    "        dir_path = path + '/' + dir\n",
    "        # Iterar por archivos\n",
    "        for f in os.listdir(dir_path):\n",
    "            image_path = dir_path + '/' + f\n",
    "            # Leer imagen\n",
    "            image = cv2.imread(image_path)\n",
    "            # Reducir el tamaño de la imagen\n",
    "            image = cv2.resize(image, (30,40), interpolation = cv2.INTER_AREA)\n",
    "            # Cambiar la imagen a blanco y negro\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            # Volver la imagen un vector\n",
    "            image = np.ravel(image)\n",
    "            cv2.destroyAllWindows()\n",
    "            # Agregar el genero de la imagen\n",
    "            X.append(image)\n",
    "            if dir == 'female':\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)\n",
    "    # X retorna los vectores de imagenes\n",
    "    # y retorna el genero de las imagenes\n",
    "    return(X, y)\n",
    "\n",
    "# Leer dataset de train\n",
    "X_train, y_train = get_images('/content/Training')\n",
    "# Leer dataset de test\n",
    "X_test, y_test = get_images('/content/Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esto se demora un rato\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Crear el SVM clasificador\n",
    "clf = svm.SVC(kernel='linear') \n",
    "\n",
    "# Entrenar el SVM\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predecir sobre el test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Revisar la precision\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taller 2, problema 3\n",
    "\n",
    "Implemente el algoritmo de substitución hacia adelante para encontrar la solución al sistema lineal Lx = b. Programar otro metodo para solucionar un sistema lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solución: [-0.73333333  0.53333333  1.13333333]\n",
      "Número de iteraciones: 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conjugate_gradient(A, b, x0, tol=1e-6, max_iter=1000):\n",
    "    x = x0\n",
    "    r = b - A @ x\n",
    "    p = r\n",
    "    iter_count = 0\n",
    "    \n",
    "    while np.linalg.norm(r) > tol and iter_count < max_iter:\n",
    "        Ap = A @ p\n",
    "        alpha = np.dot(r, r) / np.dot(p, Ap)\n",
    "        x = x + alpha * p\n",
    "        r_new = r - alpha * Ap\n",
    "        beta = np.dot(r_new, r_new) / np.dot(r, r)\n",
    "        p = r_new + beta * p\n",
    "        r = r_new\n",
    "        iter_count += 1\n",
    "    \n",
    "    return x, iter_count\n",
    "\n",
    "A = np.array([[4, 1, 3], [1, 3, 1], [3, 1, 5]])\n",
    "b = np.array([1, 2, 4])\n",
    "x0 = np.array([0, 0, 0])\n",
    "\n",
    "solucion, iteraciones = conjugate_gradient(A, b, x0)\n",
    "\n",
    "print(\"Solución:\", solucion)\n",
    "print(\"Número de iteraciones:\", iteraciones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprobemos la solucion:  [1. 2. 4.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Comprobemos la solucion: \", np.dot(A, solucion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taller 3, Problema 1\n",
    "\n",
    "Lea sobre el méto do de p otencias para aproximar el valor propio mayor de una matríz. Programar otro metodo diferente para calcular los valores propios de una matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autovalores:\n",
      "[5.29895049 4.11525727 2.58579224]\n",
      "\n",
      "Autovectores:\n",
      "[[ 0.27686126  0.81977335  0.50131776]\n",
      " [-0.67833213 -0.20278289  0.70621854]\n",
      " [ 0.6805978  -0.5355845   0.49993588]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def jacobi_method(A, epsilon=1e-10, max_iterations=1000):\n",
    "    n = len(A)\n",
    "    \n",
    "    # Inicializar la matriz P como la matriz identidad\n",
    "    P = np.identity(n)\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        # Encontrar el elemento no diagonal más grande en magnitud\n",
    "        max_val = 0.0\n",
    "        p, q = 0, 0\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                if abs(A[i, j]) > max_val:\n",
    "                    max_val = abs(A[i, j])\n",
    "                    p, q = i, j\n",
    "        \n",
    "        # Verificar convergencia\n",
    "        if max_val < epsilon:\n",
    "            break\n",
    "        \n",
    "        # Calcular el ángulo de rotación\n",
    "        theta = 0.5 * np.arctan2(2 * A[p, q], A[q, q] - A[p, p])\n",
    "        \n",
    "        # Construir la matriz de rotación\n",
    "        c = np.cos(theta)\n",
    "        s = np.sin(theta)\n",
    "        R = np.identity(n)\n",
    "        R[p, p], R[p, q], R[q, p], R[q, q] = c, -s, s, c\n",
    "        \n",
    "        # Actualizar las matrices A y P\n",
    "        A = np.dot(np.dot(R.T, A), R)\n",
    "        P = np.dot(P, R)\n",
    "    \n",
    "    # Extraer autovalores y autovectores\n",
    "    eigenvalues = np.diagonal(A)\n",
    "    eigenvectors = P\n",
    "    \n",
    "    return eigenvalues, eigenvectors\n",
    "\n",
    "# Ejemplo de uso\n",
    "# Crear una matriz simétrica real\n",
    "matrix = np.array([[4, -1, 0],\n",
    "                   [-1, 4, -1],\n",
    "                   [0, -1, 4]], dtype=float)\n",
    "\n",
    "# Aplicar el Método de Jacobi\n",
    "autovalores, autovectores = jacobi_method(matrix)\n",
    "\n",
    "print(\"Autovalores:\")\n",
    "print(autovalores)\n",
    "print(\"\\nAutovectores:\")\n",
    "print(autovectores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autovalores:\n",
      "[2.59 4.   5.41]\n",
      "\n",
      "Autovectores:\n",
      "[[ 0.5  -0.71  0.5 ]\n",
      " [ 0.71  0.   -0.71]\n",
      " [ 0.5   0.71  0.5 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Autovalores:\")\n",
    "print(np.round(np.linalg.eig(matrix)[0],2))\n",
    "print(\"\\nAutovectores:\")\n",
    "print(np.round(np.linalg.eig(matrix)[1],2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referirse al video anexo o en el github: \n",
    "\n",
    "https://github.com/coberndorm/Algebra-In-Data-Science/tree/main/Talleres/Taller_Final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
